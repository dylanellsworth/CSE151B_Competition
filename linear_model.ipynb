{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c38cf",
   "metadata": {},
   "source": [
    "## Install the package dependencies before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "16ac7530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35242f5f",
   "metadata": {},
   "source": [
    "## Use GPU if Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3b1eebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b472cf2",
   "metadata": {},
   "source": [
    "## Create a Torch.Dataset class for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "091abbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "ROOT_PATH = \"./\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "\n",
    "    \n",
    "    outputs = None\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)[:int(n * 0.8)]\n",
    "        \n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)[:int(n * 0.8)]\n",
    "        \n",
    "    elif split == 'val':\n",
    "        f_in = ROOT_PATH + 'train' + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)[int(n * 0.8):]\n",
    "        \n",
    "        f_out = ROOT_PATH + 'train' + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)[int(n * 0.8):]\n",
    "    \n",
    "    else:\n",
    "        f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "\n",
    "        # Include ALL cities in the Dataset\n",
    "        if (city == 'all'):\n",
    "            for city_name in cities:\n",
    "                city_inputs, city_outputs = get_city_trajectories(city=city_name, split=split, normalized=False)\n",
    "                for i in range(len(city_inputs)):\n",
    "                    self.inputs.append(city_inputs[i])\n",
    "                    self.outputs.append(city_outputs[i])\n",
    "        \n",
    "        # Only include the city specified\n",
    "        else:\n",
    "             self.inputs, self.outputs = get_city_trajectories(city, split=split, normalized=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = (self.inputs[idx], self.outputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "# intialize a dataset\n",
    "city = 'all' \n",
    "split = 'train'\n",
    "train_dataset  = ArgoverseDataset(city = city, split = split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058453cc",
   "metadata": {},
   "source": [
    "## Create a DataLoader class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5c14f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 4  # batch size \n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef99b32b",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee5b7ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "class Pred(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(100, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 120)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 100).float()\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = x.reshape(-1, 60, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "adbc1d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = Pred().to(device)\n",
    "opt = optim.Adam(pred.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e4725237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 1545223.1679539182\n",
      "epoch 1 loss: 681705.2476858151\n",
      "epoch 2 loss: 448073.49636376847\n",
      "epoch 3 loss: 383638.74103367404\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i_batch, sample_batch in enumerate(train_loader):\n",
    "        inp, out = sample_batch\n",
    "        inp = inp.to(device)\n",
    "        out = out.to(device)\n",
    "        preds = pred(inp)\n",
    "        loss = ((preds - out) ** 2).sum()\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print('epoch {} loss: {}'.format(epoch, total_loss / len(train_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7d6042",
   "metadata": {},
   "source": [
    "## Determine Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da57dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ArgoverseDataset(city = city, split = 'val')\n",
    "val_loader = DataLoader(val_dataset,batch_size=batch_sz)\n",
    "\n",
    "val_loss = 0\n",
    "for i_batch, sample_batch in enumerate(val_loader):\n",
    "    inp, out = sample_batch\n",
    "    inp = inp.to(device)\n",
    "    out = out.to(device)\n",
    "    preds = pred(inp)\n",
    "    loss = ((preds - out) ** 2).sum()\n",
    "\n",
    "    val_loss += loss.item()\n",
    "print('loss: {}'.format(val_loss / len(val_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80b5e4",
   "metadata": {},
   "source": [
    "## Sample a batch of data and visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6507c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "def show_sample_batch(sample_batch):\n",
    "    \"\"\"visualize the trajectory for a batch of samples\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i,:,0], inp[i,:,1])\n",
    "        axs[i].scatter(out[i,:,0], out[i,:,1])\n",
    "\n",
    "        \n",
    "for i_batch, sample_batch in enumerate(train_loader):\n",
    "    inp, out = sample_batch\n",
    "    print(inp.shape, out.shape)\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      implement your Deep learning model\n",
    "      implement training routine\n",
    "    \"\"\"\n",
    "    show_sample_batch(sample_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00333419",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "for city in cities:\n",
    "    test_input, _ = get_city_trajectories(city, 'test')\n",
    "    # print(city, \"shape:\", test_input.shape)\n",
    "    for x in range(len(test_input)):\n",
    "        traj = pred(torch.tensor(test_input[x]))\n",
    "        # print(city, \"shape:\", traj.shape)\n",
    "        # print(traj)\n",
    "        all_preds.append(traj)\n",
    "print(len(all_preds))\n",
    "print(all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02153e56",
   "metadata": {},
   "source": [
    "## Calculate Test Predictions and Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14008adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def predictions_to_csv():\n",
    "    num_pred_steps = 60\n",
    "    all_preds = np.zeros(shape=(0, num_pred_steps * 2))\n",
    "    city_col = np.array([])\n",
    "\n",
    "    # Test predictions for ALL cities\n",
    "    for city_name in cities:\n",
    "\n",
    "        print(\"Processing city\", city_name)\n",
    "\n",
    "        # Get Training input values for this city\n",
    "        test_traj_in, _ = get_city_trajectories(city=city_name, split=\"test\")\n",
    "        #print(test_traj_in.shape)\n",
    "\n",
    "        test_pred_arr = []\n",
    "        # Iterate through all test inputs for this city\n",
    "        for x in range(len(test_traj_in)):\n",
    "            # Get the predicted trajectory from the model\n",
    "            traj_in_tensor = torch.tensor(test_traj_in[x], device=device)\n",
    "            traj = pred(traj_in_tensor)[0]\n",
    "            # Add the prediction to the test prediction array\n",
    "            traj_arr = traj.cpu().detach().numpy()\n",
    "            test_pred_arr.append(traj_arr)\n",
    "        test_pred_arr = numpy.array(test_pred_arr)\n",
    "        # print(test_pred_arr.shape)\n",
    "    \n",
    "        # Reshape the predictions to the submission format size (120)\n",
    "        test_pred_arr_reshaped = np.reshape(test_pred_arr, newshape=(test_traj_in.shape[0], num_pred_steps * 2))\n",
    "        print(test_pred_arr_reshaped.shape)\n",
    "\n",
    "        # Add to total predictions / columns\n",
    "        all_preds = np.r_[all_preds, test_pred_arr_reshaped]\n",
    "        city_col = np.r_[city_col, [str(i) + \"_\" + city_name for i in range(test_pred_arr.shape[0])]]\n",
    "    \n",
    "    # Convert predictions to csv file\n",
    "    sub_df = pd.DataFrame(np.c_[city_col, all_preds], columns=[np.r_[[\"ID\"], [\"v\" + str(i) for i in range(120)]]])\n",
    "    sub_df.to_csv('predictions_submission.csv', index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe95a25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_to_csv()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e2d4884ef49a982a1eb5c43e5ae8a96487ae20917b7a89575cf03e86c2c63cb9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
