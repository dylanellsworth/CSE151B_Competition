{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c38cf",
   "metadata": {},
   "source": [
    "## Install the package dependencies before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16ac7530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60560edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b472cf2",
   "metadata": {},
   "source": [
    "## Create a Torch.Dataset class for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "091abbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ROOT_PATH = \"./\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "\n",
    "    \n",
    "    outputs = None\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)[:int(n * 0.8)]\n",
    "        \n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)[:int(n * 0.8)]\n",
    "        \n",
    "    elif split == 'val':\n",
    "        f_in = ROOT_PATH + 'train' + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)[int(n * 0.8):]\n",
    "        \n",
    "        f_out = ROOT_PATH + 'train' + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)[int(n * 0.8):]\n",
    "    \n",
    "    else:\n",
    "        f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, split:str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "\n",
    "        for city_name in cities:\n",
    "            city_inputs, city_outputs = get_city_trajectories(city=city_name, split=split, normalized=False)\n",
    "            for i in range(len(city_inputs)):\n",
    "                self.inputs.append(city_inputs[i])\n",
    "                self.outputs.append(city_outputs[i])\n",
    "        # self.inputs.to(device)\n",
    "        # self.outputs.to(device)\n",
    "        # print(len(self.inputs), len(self.outputs))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = (self.inputs[idx], self.outputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "# intialize a dataset\n",
    "train_dataset  = ArgoverseDataset(split = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a174510",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ArgoverseDataset(split = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a44d7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40765"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058453cc",
   "metadata": {},
   "source": [
    "## Create a DataLoader class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c14f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 4  # batch size \n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee5b7ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "class Pred(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(100, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 120)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 100).float()\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = x.reshape(-1, 60, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010c7c5f",
   "metadata": {},
   "source": [
    "### Define LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "aeb09181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, num_hidden=120, num_layers=4, drop=0.2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.num_hidden = num_hidden\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size = input_size*2,\n",
    "            hidden_size = num_hidden, \n",
    "            num_layers = num_layers, \n",
    "            batch_first = True,\n",
    "            dropout = drop)\n",
    "\n",
    "        self.classifier = nn.Linear(num_hidden, output_size*2)\n",
    "\n",
    "        self.h  = torch.rand(self.num_layers, output_size*2, self.num_hidden)\n",
    "        self.c = torch.rand(self.num_layers, output_size*2, self.num_hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 100).float()\n",
    "        self.lstm.flatten_parameters()\n",
    "        # print(\"x shape:\", x.shape)\n",
    "        out, (self.h, self.c) = self.lstm(x, (self.h, self.c))\n",
    "        out = self.classifier(self.h)\n",
    "        out = out.reshape(-1, self.output_size, 2)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2848aabc",
   "metadata": {},
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, num_hidden, num_layers, input_size, seq_len, batch_size, device):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.seq_len = seq_len\n",
    "        self.device = device\n",
    "        self.num_hidden = num_hidden\n",
    "        self.proj_size = 2\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(input_size=input_size, hidden_size=num_hidden, num_layers=num_layers, batch_first=True, device=device)\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear = torch.nn.Linear(in_features = num_hidden, out_features = 100, device=device)\n",
    "        self.h  = torch.rand(self.num_layers, batch_size, self.num_hidden, device=device)\n",
    "        self.c = torch.rand(self.num_layers, batch_size, self.num_hidden, device=device)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # Propagate input through LSTM\n",
    "        h  = torch.rand(self.num_layers, self.batch_size, self.num_hidden, device=device)\n",
    "        c = torch.rand(self.num_layers, self.batch_size, self.num_hidden, device=device)\n",
    "        #h2  = torch.rand(self.num_layers, self.batch_size, self.num_hidden, device=device)\n",
    "        #c2 = torch.rand(self.num_layers, self.batch_size, self.num_hidden, device=device)\n",
    "        output, (h, c) = self.lstm(x, (h, c)) #lstm with input, hidden, and internal state\n",
    "        #output, (h,c) = self.lstm2(h, (h2,c2))\n",
    "        output = self.linear(h)\n",
    "        output = torch.reshape(output, (self.batch_size, self.seq_len, self.input_size))\n",
    "#         hn = hn.view(-1, self.num_hidden) #reshaping the data for Dense layer next\n",
    "      #  //out = self.relu(hn) \n",
    "     #   //out = self.relu(out) #relu\n",
    "    #out = self.fc(out) #Final Output\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "adbc1d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = LSTM(50, 60).to(device)\n",
    "opt = optim.Adam(pred.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e4725237",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\CSE151B\\CSE151B_Competition\\LSTM_model.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CSE151B/CSE151B_Competition/LSTM_model.ipynb#ch0000010?line=5'>6</a>\u001b[0m inp \u001b[39m=\u001b[39m inp\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CSE151B/CSE151B_Competition/LSTM_model.ipynb#ch0000010?line=6'>7</a>\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/CSE151B/CSE151B_Competition/LSTM_model.ipynb#ch0000010?line=7'>8</a>\u001b[0m preds \u001b[39m=\u001b[39m pred(inp)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CSE151B/CSE151B_Competition/LSTM_model.ipynb#ch0000010?line=8'>9</a>\u001b[0m \u001b[39m# print(\"preds shape:\", preds.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CSE151B/CSE151B_Competition/LSTM_model.ipynb#ch0000010?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m ((preds \u001b[39m-\u001b[39m out) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msum()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\CSE151B\\CSE151B_Competition\\LSTM_model.ipynb Cell 12'\u001b[0m in \u001b[0;36mLSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CSE151B/CSE151B_Competition/LSTM_model.ipynb#ch0000019?line=23'>24</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm\u001b[39m.\u001b[39mflatten_parameters()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CSE151B/CSE151B_Competition/LSTM_model.ipynb#ch0000019?line=24'>25</a>\u001b[0m \u001b[39m# print(\"x shape:\", x.shape)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/CSE151B/CSE151B_Competition/LSTM_model.ipynb#ch0000019?line=25'>26</a>\u001b[0m out, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mh, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CSE151B/CSE151B_Competition/LSTM_model.ipynb#ch0000019?line=26'>27</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CSE151B/CSE151B_Competition/LSTM_model.ipynb#ch0000019?line=27'>28</a>\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_size, \u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\rnn.py:752\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/rnn.py?line=748'>749</a>\u001b[0m         \u001b[39mif\u001b[39;00m hx[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m hx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/rnn.py?line=749'>750</a>\u001b[0m             msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mFor unbatched 2-D input, hx and cx should \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/rnn.py?line=750'>751</a>\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39malso be 2-D but got (\u001b[39m\u001b[39m{\u001b[39;00mhx[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D, \u001b[39m\u001b[39m{\u001b[39;00mhx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D) tensors\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/rnn.py?line=751'>752</a>\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg)\n\u001b[0;32m    <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/rnn.py?line=752'>753</a>\u001b[0m         hx \u001b[39m=\u001b[39m (hx[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), hx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m))\n\u001b[0;32m    <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/rnn.py?line=754'>755</a>\u001b[0m \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/rnn.py?line=755'>756</a>\u001b[0m \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i_batch, sample_batch in enumerate(train_loader):\n",
    "        inp, out = sample_batch\n",
    "        inp = inp.to(device)\n",
    "        out = out.to(device)\n",
    "        preds = pred(inp)\n",
    "        # print(\"preds shape:\", preds.shape)\n",
    "\n",
    "        loss = ((preds - out) ** 2).sum()\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print('epoch {} loss: {}'.format(epoch, total_loss / len(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da57dc99",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\CSE151B\\CSE151B_Competition\\CSE151B-2022Spring-Discussion7.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CSE151B/CSE151B_Competition/CSE151B-2022Spring-Discussion7.ipynb#ch0000011?line=4'>5</a>\u001b[0m     inp, out \u001b[39m=\u001b[39m sample_batch\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CSE151B/CSE151B_Competition/CSE151B-2022Spring-Discussion7.ipynb#ch0000011?line=5'>6</a>\u001b[0m     preds \u001b[39m=\u001b[39m pred(inp)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/CSE151B/CSE151B_Competition/CSE151B-2022Spring-Discussion7.ipynb#ch0000011?line=6'>7</a>\u001b[0m     loss \u001b[39m=\u001b[39m ((preds \u001b[39m-\u001b[39;49m out) \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39msum()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CSE151B/CSE151B_Competition/CSE151B-2022Spring-Discussion7.ipynb#ch0000011?line=8'>9</a>\u001b[0m     val_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CSE151B/CSE151B_Competition/CSE151B-2022Spring-Discussion7.ipynb#ch0000011?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mloss: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(val_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(val_dataset)))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:31\u001b[0m, in \u001b[0;36m_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor.py?line=28'>29</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor.py?line=29'>30</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor.py?line=30'>31</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor.py?line=31'>32</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor.py?line=32'>33</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val_loader = DataLoader(val_dataset,batch_size=batch_sz)\n",
    "\n",
    "val_loss = 0\n",
    "for i_batch, sample_batch in enumerate(val_loader):\n",
    "    inp, out = sample_batch\n",
    "    preds = pred(inp)\n",
    "    loss = ((preds - out) ** 2).sum()\n",
    "\n",
    "    val_loss += loss.item()\n",
    "print('loss: {}'.format(val_loss / len(val_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80b5e4",
   "metadata": {},
   "source": [
    "## Sample a batch of data and visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6507c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 50, 2]) torch.Size([4, 60, 2])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x216 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "def show_sample_batch(sample_batch):\n",
    "    \"\"\"visualize the trajectory for a batch of samples\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i,:,0], inp[i,:,1])\n",
    "        axs[i].scatter(out[i,:,0], out[i,:,1])\n",
    "\n",
    "        \n",
    "for i_batch, sample_batch in enumerate(train_loader):\n",
    "    inp, out = sample_batch\n",
    "    print(inp.shape, out.shape)\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      implement your Deep learning model\n",
    "      implement training routine\n",
    "    \"\"\"\n",
    "    show_sample_batch(sample_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00333419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29843\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\CSE151B\\CSE151B_Competition\\CSE151B-2022Spring-Discussion7.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CSE151B/CSE151B_Competition/CSE151B-2022Spring-Discussion7.ipynb#ch0000014?line=8'>9</a>\u001b[0m         all_preds\u001b[39m.\u001b[39mappend(traj)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CSE151B/CSE151B_Competition/CSE151B-2022Spring-Discussion7.ipynb#ch0000014?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(all_preds))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/CSE151B/CSE151B_Competition/CSE151B-2022Spring-Discussion7.ipynb#ch0000014?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39;49m(all_preds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:305\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor.py?line=302'>303</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m\u001b[39m__repr__\u001b[39m, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor.py?line=303'>304</a>\u001b[0m \u001b[39m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor.py?line=304'>305</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_tensor_str\u001b[39m.\u001b[39;49m_str(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor_str.py:434\u001b[0m, in \u001b[0;36m_str\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor_str.py?line=431'>432</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_str\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor_str.py?line=432'>433</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor_str.py?line=433'>434</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m _str_intern(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor_str.py:409\u001b[0m, in \u001b[0;36m_str_intern\u001b[1;34m(inp)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor_str.py?line=406'>407</a>\u001b[0m                 tensor_str \u001b[39m=\u001b[39m _tensor_str(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_dense(), indent)\n\u001b[0;32m    <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor_str.py?line=407'>408</a>\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor_str.py?line=408'>409</a>\u001b[0m                 tensor_str \u001b[39m=\u001b[39m _tensor_str(\u001b[39mself\u001b[39;49m, indent)\n\u001b[0;32m    <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor_str.py?line=410'>411</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mstrided:\n\u001b[0;32m    <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor_str.py?line=411'>412</a>\u001b[0m     suffixes\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39mlayout=\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor_str.py:264\u001b[0m, in \u001b[0;36m_tensor_str\u001b[1;34m(self, indent)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor_str.py?line=261'>262</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[39mself\u001b[39m, indent, summarize, real_formatter, imag_formatter)\n\u001b[0;32m    <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor_str.py?line=262'>263</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor_str.py?line=263'>264</a>\u001b[0m     formatter \u001b[39m=\u001b[39m _Formatter(get_summarized_data(\u001b[39mself\u001b[39;49m) \u001b[39mif\u001b[39;49;00m summarize \u001b[39melse\u001b[39;49;00m \u001b[39mself\u001b[39;49m)\n\u001b[0;32m    <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor_str.py?line=264'>265</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[39mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor_str.py:92\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor_str.py?line=88'>89</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor_str.py?line=90'>91</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor_str.py?line=91'>92</a>\u001b[0m     tensor_view \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor_str.py?line=93'>94</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfloating_dtype:\n\u001b[0;32m     <a href='file:///c%3A/Users/Dylan%20Ellsworth/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/_tensor_str.py?line=94'>95</a>\u001b[0m     \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m tensor_view:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "for city in cities:\n",
    "    test_input, _ = get_city_trajectories(city, 'test')\n",
    "    # print(city, \"shape:\", test_input.shape)\n",
    "    for x in range(len(test_input)):\n",
    "        traj = pred(torch.tensor(test_input[x]))\n",
    "        # print(city, \"shape:\", traj.shape)\n",
    "        # print(traj)\n",
    "        all_preds.append(traj)\n",
    "print(len(all_preds))\n",
    "print(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14008adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing city austin\n",
      "(6325, 50, 2)\n",
      "(6325, 60, 2)\n",
      "(6325, 120)\n",
      "Processing city miami\n",
      "(7971, 50, 2)\n",
      "(7971, 60, 2)\n",
      "(7971, 120)\n",
      "Processing city pittsburgh\n",
      "(6361, 50, 2)\n",
      "(6361, 60, 2)\n",
      "(6361, 120)\n",
      "Processing city dearborn\n",
      "(3671, 50, 2)\n",
      "(3671, 60, 2)\n",
      "(3671, 120)\n",
      "Processing city washington-dc\n",
      "(3829, 50, 2)\n",
      "(3829, 60, 2)\n",
      "(3829, 120)\n",
      "Processing city palo-alto\n",
      "(1686, 50, 2)\n",
      "(1686, 60, 2)\n",
      "(1686, 120)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def predictions_to_csv():\n",
    "    num_pred_steps = 60\n",
    "    all_preds = np.zeros(shape=(0, num_pred_steps * 2))\n",
    "    city_col = np.array([])\n",
    "\n",
    "    for city_name in cities:\n",
    "\n",
    "        print(\"Processing city\", city_name)\n",
    "        # Get Training input values\n",
    "        test_traj_in, test_traj_out = get_city_trajectories(city=city_name, split=\"test\")\n",
    "        print(test_traj_in.shape)\n",
    "\n",
    "        # REPLACE. get predictions for this city\n",
    "        test_pred_arr = []\n",
    "        for x in range(len(test_traj_in)):\n",
    "            # print(\"input shape\", test_traj_in[x].shape)\n",
    "            traj = pred(torch.tensor(test_traj_in[x]))[0]\n",
    "            # print(\"output shape\", traj.shape)\n",
    "            add = traj.detach().numpy()\n",
    "            # print(add)\n",
    "            test_pred_arr.append(add)\n",
    "        test_pred_arr = numpy.array(test_pred_arr)\n",
    "        print(test_pred_arr.shape)\n",
    "    \n",
    "        # Reshape the predictions to the submission format size (120)\n",
    "        test_pred_arr_reshaped = np.reshape(test_pred_arr, newshape=(test_traj_in.shape[0], num_pred_steps * 2))\n",
    "        print(test_pred_arr_reshaped.shape)\n",
    "\n",
    "        # Add to total predictions / columns\n",
    "        all_preds = np.r_[all_preds, test_pred_arr_reshaped]\n",
    "        city_col = np.r_[city_col, [str(i) + \"_\" + city_name for i in range(test_pred_arr.shape[0])]]\n",
    "    \n",
    "    # Convert predictions to csv file\n",
    "    sub_df = pd.DataFrame(np.c_[city_col, all_preds], columns=[np.r_[[\"ID\"], [\"v\" + str(i) for i in range(120)]]])\n",
    "    sub_df.to_csv('predictions_submission.csv', index=None)\n",
    "\n",
    "\n",
    "predictions_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe95a25c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e2d4884ef49a982a1eb5c43e5ae8a96487ae20917b7a89575cf03e86c2c63cb9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
